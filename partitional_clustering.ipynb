{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitional Clustering of Countries Economic Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of this notebook is to determine which time series specific clustering algorithm is optimal for grouping the financial growth of 113 countries over the past 58 years. \n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tslearn.clustering import silhouette_score\n",
    "\n",
    "from Clustering import TimeSeriesPartitions\n",
    "\n",
    "import Constants\n",
    "\n",
    "from Utils import DataUtils, PreProcessing, VisualUtils, TimeSeriesUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "For each country, the time series of the annual evolution of its GDP per capite is considered. Data for all countries is continuously available from 1960 to 2017. Because these time series display very high variance, the logarithm of the series is taken to smoothen the results.\n",
    "\n",
    "The data is loaded from a `.csv` file to create a $113 \\times 58$ matrix. The data is then standardized and passed into a `pandas.DataFrame` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, gdp, pop, currency, map = DataUtils.load_clustering_data()\n",
    "gdp_data = np.log(gdp[:, -Constants.T:])\n",
    "pop_data = pop[:, -Constants.T:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, scaled_df, scaled_data = PreProcessing.preprocess_onlyGDP(names, gdp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Partitioning Algorithms\n",
    "\n",
    "This section gives a short description of the partitioning algorithms used in the notebook.\n",
    "\n",
    "### $k$-Means with Euclidean Distance Measure\n",
    "\n",
    "This algorithm is the standard $k$-Means algorithm using the euclidean distance measure.\n",
    "\n",
    "### $k$-Means with Dynamic Time Warping (DTW)\n",
    "\n",
    "This version of the $k$-Means algorithm has two distinct differences from the standard $k$-Means:\n",
    "- First, dynamic time warping (DTW) is used instead of euclidean distance as the distance measure. DTW is an alignment-based technique used to measure similarity between two time series. It aligns sequences non-linearly by stretching or compressing sections of the data to find the optimal match between corresponding points. \n",
    "- Second, the cluster centers (centroids) are calculated using the DTW Barycenter Averaging (DBA) algorithm. DBA computes the centroids as an average time series of the time series in the respective clusters by minimizing the total DTW distance between the average and the time series.\n",
    "\n",
    "### $k$-Shapes\n",
    "\n",
    "The $k$-Shapes algorithm is specifically designed for time series data, focusing on shape-based similarity. It uses a normalized version of cross-correlation to compare the shapes of the time series. The algorithm iteratively assigns time series to clusters by finding the shape-based centroids, which are computed preserving the temporal alignment of the data.\n",
    "\n",
    "### $k$-Medoids with DTW\n",
    "\n",
    "$k$-Medoids operates in the same way as the $k$-Means algorithm, except for the centroid calculation. This algorithm only considers elements of the dataset as potential centroids; that is, for each cluster, the algorithm selects the centroid as the time series in that cluster that minimizes the withing group sum of squared errors. $k$-Medoids is robust to outliers, in the sense that the cluster centers are not affected by outliers at all.\n",
    "\n",
    "### Kernel $k$-Means\n",
    "\n",
    "Kernel $k$-Means extends the traditional $k$-Means algorithm by operating in a higher dimensional feature space, using a kernel function. This allows the algorithm to capture non-linear trends between the data points. The algorithm applies the kernel to map the data into a higher-dimensional space, where cluster assignment and centroid updates are achieved similarly to $k$-Means. The main downside of this algorithm is that the cluster centroids in the original space are never explicitly calculated. In the context of clustering time series, the Global Alignment Kernel (GAK) is used a kernel function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the appropriate algorithm and number of clusters\n",
    "\n",
    "In order to determine which of the above algorithms is better for this task and with which number of clusters, the elbow heuristic is used. For time efficiency, we only consider $3 \\leq k \\leq 10$ clusters. Indeed, it is clear for the following elbow plot that the increase of the number of clusters to values larger than 10 are hihgly unlikely to produce improvements to the performance of the algorithms. Since this is a task of unsupervised clustering, the silhouette score measure is used to evaluate algorithm performance. \n",
    "\n",
    "As all algorithms are sensitive to initialization, each algorithm is initialized 25 times and the best performance of the 25 iterations is then kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_arr = np.arange(3, 10)\n",
    "n_init=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k$-Means with euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_km_e = []\n",
    "for k in k_arr:\n",
    "    y, _ = TimeSeriesPartitions.kmeans_euclidean(scaled_df, k, n_init)\n",
    "    scores_km_e.append(silhouette_score(scaled_df, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k$-Means with DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_km_dba = []\n",
    "for k in k_arr:\n",
    "    y, _ = TimeSeriesPartitions.kmeans_dtw(scaled_df, k, n_init)\n",
    "    scores_km_dba.append(silhouette_score(scaled_df, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k$-Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_kshape = []\n",
    "for k in k_arr:\n",
    "    y, _ = TimeSeriesPartitions.kshape(scaled_df, k, n_init)\n",
    "    scores_kshape.append(silhouette_score(scaled_df, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k$-Medoids with DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_med_dba = []\n",
    "for k in k_arr:\n",
    "    y, _ = TimeSeriesPartitions.kmedoids_dtw(scaled_df, k, n_init)\n",
    "    scores_med_dba.append(silhouette_score(scaled_df, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel $k$-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_kernel = []\n",
    "for k in k_arr:\n",
    "    y = TimeSeriesPartitions.kernel_k_means(scaled_df, k, n_init)\n",
    "    scores_kernel.append(silhouette_score(scaled_df, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.title('Elbow Plot')\n",
    "plt.plot(k_arr, scores_km_e, marker='o', linestyle='-', label='$k$-Means with Euclidean distance')\n",
    "plt.plot(k_arr, scores_km_dba, marker='o', linestyle='-', label='$k$-Means with DTW')\n",
    "plt.plot(k_arr, scores_kshape, marker='o', linestyle='-', label='k-Shapes')\n",
    "plt.plot(k_arr, scores_med_dba, marker='o', linestyle='-', label='k-Medoids with DTW')\n",
    "plt.plot(k_arr, scores_kernel, marker='o', linestyle='-', label='Kernel $k$-Means')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings of the best algorithms\n",
    "\n",
    "We now present the outcomes of the two algorithms that achieved the best performance in the previous section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-means with Euclidean distance and $k=5$ clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_km_e, cluster_centers_km_e = TimeSeriesPartitions.kmeans_euclidean(scaled_df, 5, n_init)\n",
    "\n",
    "score_km_e = silhouette_score(scaled_df, y_km_e)\n",
    "clusters_km = VisualUtils.show_clustering(\n",
    "    names, \n",
    "    5, \n",
    "    scaled_data, \n",
    "    cluster_centers_km_e, \n",
    "    y_km_e, \n",
    "    score_km_e, \n",
    "    3, \n",
    "    2, \n",
    "    \"5-Means with Euclidean distance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-Means with DTW and $k=6$ clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_km, cluster_centers_km = TimeSeriesPartitions.kmeans_dtw(scaled_df, 6, n_init)\n",
    "\n",
    "score_km = silhouette_score(scaled_df, y_km)\n",
    "clusters_km = VisualUtils.show_clustering(\n",
    "    names, \n",
    "    6, \n",
    "    scaled_data, \n",
    "    cluster_centers_km, \n",
    "    y_km, \n",
    "    score_km, \n",
    "    3, \n",
    "    2,\n",
    "    \"6-Means with DTW\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-Shapes with $k=5$ clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_s, cluster_centers_s = TimeSeriesPartitions.kshape(scaled_df, 5, n_init)\n",
    "\n",
    "score_s = silhouette_score(scaled_df, y_s)\n",
    "clusters_kmed = VisualUtils.show_clustering(\n",
    "    names, \n",
    "    5, \n",
    "    scaled_data, \n",
    "    cluster_centers_s, \n",
    "    y_s, \n",
    "    score_s, \n",
    "    3, \n",
    "    2,\n",
    "    \"5-Shape\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-Medoids with DTW and $k=5$ clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmed, cluster_centers_kmed = TimeSeriesPartitions.kmedoids_dtw(scaled_df, 5, n_init)\n",
    "\n",
    "score_kmed = silhouette_score(scaled_df, y_kmed)\n",
    "clusters_kmed = VisualUtils.show_clustering(\n",
    "    names, \n",
    "    5, \n",
    "    scaled_data, \n",
    "    cluster_centers_kmed, \n",
    "    y_kmed, \n",
    "    score_kmed, \n",
    "    3, \n",
    "    2,\n",
    "    \"5-Medoids with DTW\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel $k$-means with GAK and $k=6$ clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_k = TimeSeriesPartitions.kernel_k_means(scaled_df, 6, n_init)\n",
    "cluster_centers_k = TimeSeriesUtils.cluster_centroids(scaled_data, 6, y_k, T)\n",
    "score_k = silhouette_score(scaled_df, y_k)\n",
    "clusters_kmed = VisualUtils.show_clustering(\n",
    "    names, \n",
    "    6, \n",
    "    scaled_data, \n",
    "    cluster_centers_k, \n",
    "    y_k, \n",
    "    score_k, \n",
    "    3, \n",
    "    2,\n",
    "    \"Kernel 6-Means with GAK\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization on Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_path = \"Data/locations.csv\"\n",
    "locations = DataUtils.load_locations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VisualUtils.show_clusters_on_map(names, y_km_e, map, '5-Means with Euclidean Distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VisualUtils.show_clusters_on_map(names, y_km, map, '6-Means with Dynamic Time Warping Measure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VisualUtils.show_clusters_on_map(names, y_s, map, '5-Shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VisualUtils.show_clusters_on_map(names, y_k, map, 'Kernel 6-Means with GAK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Processing\n",
    "\n",
    "It appears that kernel $6$-Means is the algorithm that produces the best structured clusters. We display how this partition relates to several types of commonly accepted groups of countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = DataUtils.load_groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import PostProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PostProcessing.postprocess_clustering(scaled_data, locations, names, groups, y_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The best suited algorithm for this task is kernel $k$-means with $k=6$ clusters.\n",
    " \n",
    "Overall, it looks like the partitioning algorithms specifically designed for time series clustering outperform the hierarchical and spectral clustering apporaches taken in this project. The fact that the cluster centroids are calculated as time series for all algorithms apart from kernel $k$-Means is very useful when considering the end goal of the project, which is to tune one regressive neural network model for each cluster. This study also produces the ideas of explicitly calculating the cluster barycenters using DBA for other approached that do not use centroids in the feature space (i.e. hierarchical clustering) and of using DTW in the context of spectral clustering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
